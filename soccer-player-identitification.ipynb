{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-28T08:17:49.816731Z",
     "iopub.status.busy": "2025-06-28T08:17:49.816519Z",
     "iopub.status.idle": "2025-06-28T08:17:52.839386Z",
     "shell.execute_reply": "2025-06-28T08:17:52.838594Z",
     "shell.execute_reply.started": "2025-06-28T08:17:49.816708Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q ultralytics\n",
    "!pip install -q supervision\n",
    "!pip install -q opencv-python\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q torchreid\n",
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-28T08:17:52.842160Z",
     "iopub.status.busy": "2025-06-28T08:17:52.841891Z",
     "iopub.status.idle": "2025-06-28T08:17:57.783018Z",
     "shell.execute_reply": "2025-06-28T08:17:57.782318Z",
     "shell.execute_reply.started": "2025-06-28T08:17:52.842122Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-wcfh9_6n\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-wcfh9_6n\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Save video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:17:57.786475Z",
     "iopub.status.busy": "2025-06-28T08:17:57.786235Z",
     "iopub.status.idle": "2025-06-28T08:17:58.065235Z",
     "shell.execute_reply": "2025-06-28T08:17:58.064451Z",
     "shell.execute_reply.started": "2025-06-28T08:17:57.786435Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "def read_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path) #capture the given video\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"❌ Could not open video: {video_path}\")\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: # means if ret is false which tells that current thing is not a frame\n",
    "            break # loop will break and video has ended\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def save_video(output_video_frames, output_video_path):\n",
    "    #output_video_frames is list of frames\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID') #format of video \n",
    "    # making the video\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 25, (output_video_frames[0].shape[1], output_video_frames[0].shape[0]))\n",
    "    # output_video_frames[0].shape[1], output_video_frames[0].shape[0] --> height and width of frame\n",
    "    for frame in output_video_frames:\n",
    "                          out.write(frame) # write the frame to the videowriter\n",
    "    out.release()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:17:58.066263Z",
     "iopub.status.busy": "2025-06-28T08:17:58.066046Z",
     "iopub.status.idle": "2025-06-28T08:17:58.070372Z",
     "shell.execute_reply": "2025-06-28T08:17:58.069831Z",
     "shell.execute_reply.started": "2025-06-28T08:17:58.066247Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_center_of_bbox(bbox):\n",
    "        x1,y1,x2,y2 = bbox\n",
    "        return int((x1+x2)/2),int((y1+y2)/2)\n",
    "\n",
    "    \n",
    "    \n",
    "def get_bbox_width(bbox):\n",
    "        return bbox[2] - bbox[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Of Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:17:58.071282Z",
     "iopub.status.busy": "2025-06-28T08:17:58.071051Z",
     "iopub.status.idle": "2025-06-28T08:18:01.503435Z",
     "shell.execute_reply": "2025-06-28T08:18:01.502685Z",
     "shell.execute_reply.started": "2025-06-28T08:17:58.071257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sys \n",
    "\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = YOLO(model_path) \n",
    "        self.tracker = sv.ByteTrack()\n",
    "\n",
    "    \n",
    "\n",
    "    def detect_frames(self, frames):\n",
    "        batch_size=20 \n",
    "        detections = [] \n",
    "        for i in range(0,len(frames),batch_size):\n",
    "            detections_batch = self.model.predict(frames[i:i+batch_size],conf=0.1)\n",
    "            detections += detections_batch\n",
    "        return detections\n",
    "\n",
    "    def get_object_tracks(self, frames, read_from_stub=False, stub_path=None):\n",
    "        \n",
    "        if read_from_stub and stub_path is not None and os.path.exists(stub_path):\n",
    "            with open(stub_path,'rb') as f:\n",
    "                tracks = pickle.load(f)\n",
    "            return tracks\n",
    "\n",
    "        detections = self.detect_frames(frames)\n",
    "\n",
    "        tracks={\n",
    "            \"players\":[],\n",
    "            \"referees\":[],\n",
    "            \"ball\":[]\n",
    "        }\n",
    "\n",
    "        for frame_num, detection in enumerate(detections):\n",
    "            cls_names = detection.names\n",
    "            cls_names_inv = {v:k for k,v in cls_names.items()}\n",
    "\n",
    "            # Covert to supervision Detection format\n",
    "            detection_supervision = sv.Detections.from_ultralytics(detection)\n",
    "\n",
    "            # Convert GoalKeeper to player object\n",
    "            for object_ind , class_id in enumerate(detection_supervision.class_id):\n",
    "                if cls_names[class_id] == \"goalkeeper\":\n",
    "                    detection_supervision.class_id[object_ind] = cls_names_inv[\"player\"]\n",
    "\n",
    "            # Track Objects\n",
    "            detection_with_tracks = self.tracker.update_with_detections(detection_supervision)\n",
    "\n",
    "            tracks[\"players\"].append({})\n",
    "            tracks[\"referees\"].append({})\n",
    "            tracks[\"ball\"].append({})\n",
    "\n",
    "            for frame_detection in detection_with_tracks:\n",
    "                bbox = frame_detection[0].tolist()\n",
    "                cls_id = frame_detection[3]\n",
    "                track_id = frame_detection[4]\n",
    "\n",
    "                if cls_id == cls_names_inv['player']:\n",
    "                    tracks[\"players\"][frame_num][track_id] = {\"bbox\":bbox}\n",
    "                \n",
    "                if cls_id == cls_names_inv['referee']:\n",
    "                    tracks[\"referees\"][frame_num][track_id] = {\"bbox\":bbox}\n",
    "            \n",
    "            for frame_detection in detection_supervision:\n",
    "                bbox = frame_detection[0].tolist()\n",
    "                cls_id = frame_detection[3]\n",
    "\n",
    "                if cls_id == cls_names_inv['ball']:\n",
    "                    tracks[\"ball\"][frame_num][1] = {\"bbox\":bbox}\n",
    "\n",
    "        if stub_path is not None:\n",
    "            with open(stub_path,'wb') as f:\n",
    "                pickle.dump(tracks,f)\n",
    "\n",
    "        return tracks\n",
    "\n",
    "    \n",
    "    def draw_traingle(self,frame,bbox,color):\n",
    "        y= int(bbox[1])\n",
    "        x,_ = get_center_of_bbox(bbox)\n",
    "\n",
    "        triangle_points = np.array([\n",
    "            [x,y],\n",
    "            [x-10,y-20],\n",
    "            [x+10,y-20],\n",
    "        ])\n",
    "        cv2.drawContours(frame, [triangle_points],0,color, cv2.FILLED)\n",
    "        cv2.drawContours(frame, [triangle_points],0,(0,0,0), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    \n",
    "    def draw_ellipse(self,frame,bbox,color,track_id=None):\n",
    "        y2 = int(bbox[3])\n",
    "        x_center, _ = get_center_of_bbox(bbox)\n",
    "        width = get_bbox_width(bbox)\n",
    "\n",
    "        cv2.ellipse(\n",
    "            frame,\n",
    "            center=(x_center,y2),\n",
    "            axes=(int(width), int(0.35*width)),\n",
    "            angle=0.0,\n",
    "            startAngle=-45,\n",
    "            endAngle=235,\n",
    "            color = color,\n",
    "            thickness=2,\n",
    "            lineType=cv2.LINE_4\n",
    "        )\n",
    "\n",
    "        rectangle_width = 40\n",
    "        rectangle_height=20\n",
    "        x1_rect = x_center - rectangle_width//2\n",
    "        x2_rect = x_center + rectangle_width//2\n",
    "        y1_rect = (y2- rectangle_height//2) +15\n",
    "        y2_rect = (y2+ rectangle_height//2) +15\n",
    "\n",
    "        if track_id is not None:\n",
    "            cv2.rectangle(frame,\n",
    "                          (int(x1_rect),int(y1_rect) ),\n",
    "                          (int(x2_rect),int(y2_rect)),\n",
    "                          color,\n",
    "                          cv2.FILLED)\n",
    "            \n",
    "            x1_text = x1_rect+12\n",
    "            if track_id > 99:\n",
    "                x1_text -=10\n",
    "            \n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"{track_id}\",\n",
    "                (int(x1_text),int(y1_rect+15)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0,0,0),\n",
    "                2\n",
    "            )\n",
    "\n",
    "        return frame\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def draw_annotations(self,video_frames, tracks):\n",
    "        output_video_frames= []\n",
    "        for frame_num, frame in enumerate(video_frames):\n",
    "            frame = frame.copy()\n",
    "\n",
    "            player_dict = tracks[\"players\"][frame_num]\n",
    "            ball_dict = tracks[\"ball\"][frame_num]\n",
    "            referee_dict = tracks[\"referees\"][frame_num]\n",
    "\n",
    "            # Draw Players\n",
    "            for track_id, player in player_dict.items():\n",
    "                frame = self.draw_ellipse(frame, player['bbox'],(0,0,255), track_id)\n",
    "\n",
    "            # Draw Referee\n",
    "            for _, referee in referee_dict.items():\n",
    "                frame = self.draw_ellipse(frame, referee[\"bbox\"],(0,255,255))\n",
    "                \n",
    "            # Draw ball \n",
    "            for track_id, ball in ball_dict.items():\n",
    "                frame = self.draw_traingle(frame, ball[\"bbox\"],(0,255,0))\n",
    "            \n",
    "\n",
    "            output_video_frames.append(frame)\n",
    "\n",
    "        return output_video_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Video\n",
    "video_frames = read_video('/kaggle/input/football-clip/15sec_input_720p.mp4')\n",
    "print(f'number of frames : {len(video_frames)}')\n",
    "# Initialize Tracker\n",
    "tracker = Tracker('/kaggle/input/best.pt/pytorch/default/1/best.pt')\n",
    "\n",
    "tracks = tracker.get_object_tracks(video_frames,\n",
    "                                   read_from_stub=True,\n",
    "                                   stub_path='/kaggle/working/track_players.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:18:02.828355Z",
     "iopub.status.busy": "2025-06-28T08:18:02.828056Z",
     "iopub.status.idle": "2025-06-28T08:18:02.844997Z",
     "shell.execute_reply": "2025-06-28T08:18:02.844330Z",
     "shell.execute_reply.started": "2025-06-28T08:18:02.828331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/kaggle/input/player-id/track_players.pkl','rb') as f:\n",
    "    tracks = pickle.load(f)\n",
    "\n",
    "len(tracks['players']) #number of frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing Players Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:18:02.846062Z",
     "iopub.status.busy": "2025-06-28T08:18:02.845815Z",
     "iopub.status.idle": "2025-06-28T08:18:03.268852Z",
     "shell.execute_reply": "2025-06-28T08:18:03.268319Z",
     "shell.execute_reply.started": "2025-06-28T08:18:02.846045Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/players_images\"\n",
    "\n",
    "# Iterate over frames\n",
    "for frame_idx, frame_players in enumerate(tracks['players']):\n",
    "    # Make a directory for this frame\n",
    "    frame_folder = os.path.join(output_dir, f\"frame_{frame_idx:04d}\")\n",
    "    os.makedirs(frame_folder, exist_ok=True)\n",
    "    \n",
    "    frame = video_frames[frame_idx]\n",
    "\n",
    "    for track_id, player in frame_players.items():\n",
    "        bbox = player['bbox']\n",
    "        \n",
    "        # Crop the player from the frame\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cropped_image = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Save the cropped image in the current frame's folder\n",
    "        save_path = os.path.join(frame_folder, f\"player_{track_id}.jpg\")\n",
    "        cv2.imwrite(save_path, cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reidentification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:57:41.965184Z",
     "iopub.status.busy": "2025-06-28T08:57:41.964568Z",
     "iopub.status.idle": "2025-06-28T08:57:41.976763Z",
     "shell.execute_reply": "2025-06-28T08:57:41.976079Z",
     "shell.execute_reply.started": "2025-06-28T08:57:41.965160Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import clip  \n",
    "\n",
    "def assign_consistent_ids_clip(video_frames, tracks, cropped_dir, threshold):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load CLIP model and preprocessing\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model.eval()\n",
    "\n",
    "    reid = {}  # global_id: feature_vector\n",
    "    # update_track_id's\n",
    "    updated_track = {}\n",
    "    updated_track['players'] = []\n",
    "    updated_track['referees'] = tracks['referees']\n",
    "    updated_track['ball'] = tracks['ball']\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(len(video_frames)):\n",
    "        # entering the first frame folder\n",
    "        frame_folder = os.path.join(cropped_dir, f\"frame_{i:04d}\")\n",
    "        # output = /kaggle/working/players_images/frame_0000\n",
    "        \n",
    "        # creating a dictionary inside the list updated_tracks['players'] for each frame\n",
    "        updated_track['players'].append({}) # creating 1 element of list that is dictionary \n",
    "        for filename in os.listdir(frame_folder):\n",
    "\n",
    "            \n",
    "            if not filename.endswith(\".jpg\"):\n",
    "                continue\n",
    "            # output - player_17.jpg\n",
    "            \n",
    "            # Original track ID\n",
    "            track_id = int(filename.split('_')[1].split('.')[0])\n",
    "            # output - 17\n",
    "\n",
    "            # Image_path\n",
    "            img_path = os.path.join(frame_folder, filename)\n",
    "            #output - /kaggle/working/players_images/frame_0000/player_17.jpg\n",
    "\n",
    "            # Preprocess and extract CLIP feature\n",
    "            try:\n",
    "                image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feature = model.encode_image(image)\n",
    "                feature = feature / feature.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "\n",
    "            feature_np = feature.cpu().numpy() # shape (1,512)\n",
    "            feature_vector = feature_np.flatten() # shape(512)\n",
    "            \n",
    "            # First frame \n",
    "            if i == 0:\n",
    "                reid[track_id] = feature_vector\n",
    "                updated_track['players'][i][track_id] = tracks['players'][i][track_id]\n",
    "            \n",
    "            else:\n",
    "                matched_id = None\n",
    "               # Step 1: Direct match (track_id seen before)  \n",
    "                matched_id = None\n",
    "                if track_id in reid:\n",
    "                    matched_id = track_id\n",
    "                    \n",
    "                else:\n",
    "                    max_sim = 0 # max similarity score\n",
    "                    # Step 2: Check for similarity score with existing global IDs\n",
    "                    for rid, ref_feat in reid.items():\n",
    "                        sim = cosine_similarity(feature_np, ref_feat.reshape(1, -1))[0][0]\n",
    "                        if sim >= threshold:\n",
    "                            if sim > max_sim:\n",
    "                                max_sim = sim # update the max_sim score\n",
    "                                matched_id = rid\n",
    "                            \n",
    "\n",
    "                \n",
    "                # Step 3: Assign and update\n",
    "                if matched_id is not None:\n",
    "                    updated_track['players'][i][matched_id] = tracks['players'][i][track_id]\n",
    "                    \n",
    "                else:\n",
    "                    new_id = max(reid.keys(), default=0) + 1\n",
    "                    reid[new_id] = feature_vector\n",
    "                    updated_track['players'][i][new_id] = tracks['players'][i][track_id]\n",
    "                \n",
    "   # saving the file\n",
    "    import pickle\n",
    "\n",
    "    with open('/kaggle/working/updated_track_id.pkl', 'wb') as f:\n",
    "        pickle.dump(updated_track, f)\n",
    "\n",
    "    with open('/kaggle/working/reid.pkl', 'wb') as f:\n",
    "        pickle.dump(reid, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS_Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''import torch\n",
    "import torchreid\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load model\n",
    "model = torchreid.models.build_model(\n",
    "    name='osnet_x1_0',\n",
    "    num_classes=1000,  \n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "state_dict = torch.load(\"/kaggle/input/os_net/pytorch/default/1/osnet_x1_0_imagenet.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "model\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import clip  # make sure clip is installed: !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "def assign_consistent_ids_clip(video_frames, tracks, cropped_dir, threshold):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    \n",
    "\n",
    "    reid = {}  # global_id: feature_vector\n",
    "    updated_track = {}\n",
    "    updated_track['players'] = []\n",
    "    updated_track['referees'] = tracks['referees']\n",
    "    updated_track['ball'] = tracks['ball']\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(len(video_frames)):\n",
    "        # entering the first frame folder\n",
    "        frame_folder = os.path.join(cropped_dir, f\"frame_{i:04d}\")\n",
    "        # output = /kaggle/working/players_images/frame_0000\n",
    "        \n",
    "        # creating a dictionary inside the list updated_tracks['players'] for each frame\n",
    "        updated_track['players'].append({}) # creating 1 element of list that is dictionary \n",
    "        for filename in os.listdir(frame_folder):\n",
    "\n",
    "            \n",
    "            if not filename.endswith(\".jpg\"):\n",
    "                continue\n",
    "            # output - player_17.jpg\n",
    "            \n",
    "            # Original track ID\n",
    "            track_id = int(filename.split('_')[1].split('.')[0])\n",
    "            # output - 17\n",
    "\n",
    "            # Image_path\n",
    "            img_path = os.path.join(frame_folder, filename)\n",
    "            #output - /kaggle/working/players_images/frame_0000/player_17.jpg\n",
    "\n",
    "            # Preprocess and extract CLIP feature\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feature = model(img_tensor)\n",
    "                \n",
    "            \n",
    "            feature_np = feature.cpu().numpy()\n",
    "            feature_vector = feature_np.flatten()\n",
    "            if i == 0:\n",
    "                reid[track_id] = feature_vector\n",
    "                updated_track['players'][i][track_id] = tracks['players'][i][track_id]\n",
    "            \n",
    "            else:\n",
    "                matched_id = None\n",
    "                \n",
    "                # Step 1: Direct match (track_id seen before)    \n",
    "                if track_id in reid:\n",
    "                    matched_id = track_id\n",
    "                    #if i == 2:\n",
    "                        #print(f\"Frame {i}, replacing track_id {track_id} with consistent ID {matched_id}\")\n",
    "                else:\n",
    "                    # Step 2: Check for similarity with existing IDs\n",
    "                    for rid, ref_feat in reid.items():\n",
    "                        sim = cosine_similarity(feature_np, ref_feat.reshape(1, -1))[0][0]\n",
    "                        if sim >= threshold:\n",
    "                            matched_id = rid\n",
    "                            #if i == 2:\n",
    "                                #print(f\"Frame {i}, replacing track_id {track_id} with consistent ID {matched_id}, sim = {sim:.2f}\")\n",
    "                            break\n",
    "                # Step 3: Assign and update\n",
    "                if matched_id is not None:\n",
    "                    updated_track['players'][i][matched_id] = tracks['players'][i][track_id]\n",
    "                else:\n",
    "                    new_id = max(reid.keys(), default=0) + 1\n",
    "                    reid[new_id] = feature_vector\n",
    "                    updated_track['players'][i][new_id] = tracks['players'][i][track_id]\n",
    "                \n",
    "            \n",
    "\n",
    "                    \n",
    "        \n",
    "\n",
    "   # saving the file\n",
    "    import pickle\n",
    "\n",
    "    with open('/kaggle/working/updated_track_id.pkl', 'wb') as f:\n",
    "        pickle.dump(updated_track, f)\n",
    "\n",
    "    with open('/kaggle/working/reid.pkl', 'wb') as f:\n",
    "        pickle.dump(reid, f)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:57:44.874879Z",
     "iopub.status.busy": "2025-06-28T08:57:44.874591Z",
     "iopub.status.idle": "2025-06-28T08:59:18.245262Z",
     "shell.execute_reply": "2025-06-28T08:59:18.244594Z",
     "shell.execute_reply.started": "2025-06-28T08:57:44.874857Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_tracks = assign_consistent_ids_clip(\n",
    "    video_frames=video_frames,\n",
    "    tracks=tracks,\n",
    "    cropped_dir=\"/kaggle/working/players_images\",\n",
    "    threshold=0.85\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening the update track_id file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:59:18.246644Z",
     "iopub.status.busy": "2025-06-28T08:59:18.246408Z",
     "iopub.status.idle": "2025-06-28T08:59:18.254229Z",
     "shell.execute_reply": "2025-06-28T08:59:18.253551Z",
     "shell.execute_reply.started": "2025-06-28T08:59:18.246620Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/kaggle/working/updated_track_id.pkl', 'rb') as f:\n",
    "    updated_tracks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:59:18.255251Z",
     "iopub.status.busy": "2025-06-28T08:59:18.254947Z",
     "iopub.status.idle": "2025-06-28T08:59:18.271676Z",
     "shell.execute_reply": "2025-06-28T08:59:18.271167Z",
     "shell.execute_reply.started": "2025-06-28T08:59:18.255235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_tracks['players'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:59:18.272976Z",
     "iopub.status.busy": "2025-06-28T08:59:18.272793Z",
     "iopub.status.idle": "2025-06-28T08:59:18.286504Z",
     "shell.execute_reply": "2025-06-28T08:59:18.285888Z",
     "shell.execute_reply.started": "2025-06-28T08:59:18.272954Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/kaggle/working/reid.pkl', 'rb') as f:\n",
    "    reid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:59:18.287402Z",
     "iopub.status.busy": "2025-06-28T08:59:18.287181Z",
     "iopub.status.idle": "2025-06-28T08:59:18.302457Z",
     "shell.execute_reply": "2025-06-28T08:59:18.301696Z",
     "shell.execute_reply.started": "2025-06-28T08:59:18.287377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([16, 8, 13, 11, 14, 12, 5, 7, 9, 2, 4, 3, 10, 1, 6, 17, 18, 19, 20, 21, 22, 23])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reid.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:59:28.505969Z",
     "iopub.status.busy": "2025-06-28T08:59:28.505665Z",
     "iopub.status.idle": "2025-06-28T08:59:31.509293Z",
     "shell.execute_reply": "2025-06-28T08:59:31.508594Z",
     "shell.execute_reply.started": "2025-06-28T08:59:28.505939Z"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "tracker = Tracker('/kaggle/input/best.pt/pytorch/default/1/best.pt')\n",
    "ouput_video_frames = tracker.draw_annotations(video_frames, updated_tracks)\n",
    "# Save video\n",
    "save_video(ouput_video_frames, '/kaggle/working/output_video.avi') # same video just as output"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7730158,
     "sourceId": 12267129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7753152,
     "sourceId": 12300792,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7755355,
     "sourceId": 12303957,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 384330,
     "modelInstanceId": 363464,
     "sourceId": 447709,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 387204,
     "modelInstanceId": 366311,
     "sourceId": 451508,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
